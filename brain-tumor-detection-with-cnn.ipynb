{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":377107,"sourceType":"datasetVersion","datasetId":165566}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brain Tumor Detection with CNN","metadata":{}},{"cell_type":"code","source":"import numpy as np  \nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport warnings\nwarnings.filterwarnings('ignore')\nimport cv2\nimport glob","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-09T09:18:16.522733Z","iopub.execute_input":"2024-04-09T09:18:16.523233Z","iopub.status.idle":"2024-04-09T09:18:19.556424Z","shell.execute_reply.started":"2024-04-09T09:18:16.523201Z","shell.execute_reply":"2024-04-09T09:18:19.555503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Paths","metadata":{}},{"cell_type":"code","source":"path_No = '/kaggle/input/brain-mri-images-for-brain-tumor-detection/no/*'\npath_Yes = '/kaggle/input/brain-mri-images-for-brain-tumor-detection/yes/*'","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:19.557846Z","iopub.execute_input":"2024-04-09T09:18:19.558289Z","iopub.status.idle":"2024-04-09T09:18:19.563454Z","shell.execute_reply.started":"2024-04-09T09:18:19.558267Z","shell.execute_reply":"2024-04-09T09:18:19.562365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  Initialize Empty Lists","metadata":{}},{"cell_type":"code","source":"tumor = []\nno_tumor = []","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:19.564623Z","iopub.execute_input":"2024-04-09T09:18:19.565431Z","iopub.status.idle":"2024-04-09T09:18:19.579108Z","shell.execute_reply.started":"2024-04-09T09:18:19.565396Z","shell.execute_reply":"2024-04-09T09:18:19.578392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* tumor: This list will store tuples containing the preprocessed image and a label (1 for tumor).\n* no_tumor: This list will store tuples containing the preprocessed image and a label (0 for no tumor).","metadata":{}},{"cell_type":"markdown","source":"# Load and Preprocess Images (Yes Folder)","metadata":{}},{"cell_type":"code","source":"# sets a seed for random number generation, ensuring some reproducibility in the data preprocessing steps (useful for splitting data later).\nrandom_state = 42\n\nfor file in glob.iglob(path_Yes):\n    img = cv2.imread(file)      \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)        \n    img = cv2.resize(img, (224, 224)) \n    tumor.append((img, 1)) ","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:19.581404Z","iopub.execute_input":"2024-04-09T09:18:19.581913Z","iopub.status.idle":"2024-04-09T09:18:20.253699Z","shell.execute_reply.started":"2024-04-09T09:18:19.581886Z","shell.execute_reply":"2024-04-09T09:18:20.252598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* This loop iterates over all image files in the \"yes\" folder using glob.iglob.\n* cv2.imread(file): Reads the image from the file path.\n* cv2.cvtColor(img, cv2.COLOR_BGR2RGB): Converts the image from BGR (OpenCV's default format) to RGB format, which is commonly used by deep learning models.\n* cv2.resize(img, (224, 224)): Resizes the image to a standard size of 224x224 pixels. This is often a requirement for pre-trained models or for maintaining consistency within the dataset.\n* tumor.append((img, 1)): The preprocessed image along with its label (1 for tumor) is appended to the tumor list.","metadata":{}},{"cell_type":"markdown","source":"# Load and Preprocess Images (No Folder)","metadata":{}},{"cell_type":"code","source":"for file in glob.iglob(path_No):\n    img = cv2.imread(file)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (224, 224))\n    no_tumor.append((img, 0)) ","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:20.254808Z","iopub.execute_input":"2024-04-09T09:18:20.25503Z","iopub.status.idle":"2024-04-09T09:18:20.633546Z","shell.execute_reply.started":"2024-04-09T09:18:20.25501Z","shell.execute_reply":"2024-04-09T09:18:20.632877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*This loop performs similar operations as the previous loop but iterates over the \"no\" folder and appends images with a label of 0 (no tumor) to the no_tumor list.*","metadata":{}},{"cell_type":"markdown","source":"# Combine Data","metadata":{}},{"cell_type":"code","source":"all_data = tumor + no_tumor","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:20.634469Z","iopub.execute_input":"2024-04-09T09:18:20.634751Z","iopub.status.idle":"2024-04-09T09:18:20.63859Z","shell.execute_reply.started":"2024-04-09T09:18:20.634731Z","shell.execute_reply":"2024-04-09T09:18:20.637727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*This line combines the images and labels from both lists into a single list called all_data.*","metadata":{}},{"cell_type":"markdown","source":"# Extract Images and Labels as NumPy arrays","metadata":{}},{"cell_type":"code","source":"data = np.array([item[0] for item in all_data])\nlabels = np.array([item[1] for item in all_data])","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:20.639867Z","iopub.execute_input":"2024-04-09T09:18:20.640496Z","iopub.status.idle":"2024-04-09T09:18:20.654498Z","shell.execute_reply.started":"2024-04-09T09:18:20.640471Z","shell.execute_reply":"2024-04-09T09:18:20.653517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* data: This creates a NumPy array containing only the image data (the first element of each tuple in all_data).\n* labels: This creates a NumPy array containing only the labels (the second element of each tuple in all_data). Converting to NumPy arrays is essential for efficient processing and training within deep learning frameworks like TensorFlow or Keras.","metadata":{}},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"# displays a single image from the data \nplt.imshow(data[20])","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:20.655833Z","iopub.execute_input":"2024-04-09T09:18:20.656219Z","iopub.status.idle":"2024-04-09T09:18:20.901141Z","shell.execute_reply.started":"2024-04-09T09:18:20.65619Z","shell.execute_reply":"2024-04-09T09:18:20.90027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualizing a Grid of Random Images\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Define number of rows and columns for the grid\nnum_rows = 5\nnum_cols = 5\n\n# Randomly select indices for images\nimage_indices = np.random.randint(0, len(data), size=num_rows * num_cols)\n\n# Create a figure and subplots\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n\n# Plot random images\nfor i in range(num_rows):\n  for j in range(num_cols):\n    index = image_indices[i * num_cols + j]\n    axes[i, j].imshow(data[index])\n    axes[i, j].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:20.902179Z","iopub.execute_input":"2024-04-09T09:18:20.903151Z","iopub.status.idle":"2024-04-09T09:18:22.062174Z","shell.execute_reply.started":"2024-04-09T09:18:20.903119Z","shell.execute_reply":"2024-04-09T09:18:22.06133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Data into Training, Validation, and Testing Sets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=random_state)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=random_state)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:22.065541Z","iopub.execute_input":"2024-04-09T09:18:22.066149Z","iopub.status.idle":"2024-04-09T09:18:22.537265Z","shell.execute_reply.started":"2024-04-09T09:18:22.066124Z","shell.execute_reply":"2024-04-09T09:18:22.536407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We import train_test_split from sklearn.model_selection for data splitting.\n* train_test_split splits the data (X - images and y - labels) into training and testing sets with a test size of 20% (test_size=0.2). We set random_state for reproducibility.\n* We further split the training data (X_train and y_train) into training and validation sets with a validation size of 10% (test_size=0.1). This creates a hold-out validation set to monitor model performance during training and prevent overfitting.","metadata":{}},{"cell_type":"markdown","source":"# Normalize the Image Data ","metadata":{}},{"cell_type":"code","source":"X_train = X_train / 255.0\nX_val = X_val / 255.0\nX_test = X_test / 255.0","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:22.53827Z","iopub.execute_input":"2024-04-09T09:18:22.539466Z","iopub.status.idle":"2024-04-09T09:18:22.588465Z","shell.execute_reply.started":"2024-04-09T09:18:22.539443Z","shell.execute_reply":"2024-04-09T09:18:22.587832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(y_train, bins=2)  # Bins for tumor (1) and no tumor (0)\nplt.xlabel('Class Label')\nplt.ylabel('Number of Images')\nplt.title('Class Distribution in Training Data')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:22.589334Z","iopub.execute_input":"2024-04-09T09:18:22.589998Z","iopub.status.idle":"2024-04-09T09:18:22.76884Z","shell.execute_reply.started":"2024-04-09T09:18:22.589977Z","shell.execute_reply":"2024-04-09T09:18:22.767884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creates a histogram using plt.hist to visualize the distribution of labels (tumor vs no tumor) in the training data.\n\n* bins=2: This argument specifies the number of bins (bars) in the histogram. Here, we set it to 2 because we only have two classes (tumor and no tumor).\n* plt.xlabel('Class Label'): This sets the label for the x-axis, indicating it represents the class labels.\n\n* plt.ylabel('Number of Images'): This sets the label for the y-axis, indicating it represents the count of images in each class.","metadata":{}},{"cell_type":"markdown","source":"# Build the CNN Model using Sequential API","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\n\nmodel = keras.Sequential([\n  keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n  keras.layers.MaxPooling2D((2, 2)),\n  keras.layers.Conv2D(64, (3, 3), activation='relu'),\n  keras.layers.MaxPooling2D((2, 2)),\n  keras.layers.Flatten(),\n  keras.layers.Dense(64, activation='relu'),\n  keras.layers.Dense(1, activation='sigmoid')\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:22.769785Z","iopub.execute_input":"2024-04-09T09:18:22.770414Z","iopub.status.idle":"2024-04-09T09:18:22.879675Z","shell.execute_reply.started":"2024-04-09T09:18:22.770391Z","shell.execute_reply":"2024-04-09T09:18:22.87859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Conv2D Layers: These convolutional layers extract features from the images.\n* The first layer has 32 filters of size 3x3 and uses ReLU activation.\n* The second layer has 64 filters of size 3x3 and also uses ReLU activation.\n* MaxPooling2D Layers: These layers downsample the data to reduce dimensionality and control overfitting.\n* Flatten Layer: This layer flattens the multi-dimensional output of the convolutional layers into a single-dimensional vector suitable for feeding into dense layers.\n* Dense Layers: These are fully connected layers that perform traditional neural network operations.\n* The first dense layer has 64 neurons and ReLU activation.\n* The final dense layer has 1 neuron and sigmoid activation for binary classification (tumor or no tumor).","metadata":{}},{"cell_type":"markdown","source":"# Compile the Model","metadata":{}},{"cell_type":"code","source":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:22.880712Z","iopub.execute_input":"2024-04-09T09:18:22.880942Z","iopub.status.idle":"2024-04-09T09:18:22.891472Z","shell.execute_reply.started":"2024-04-09T09:18:22.880922Z","shell.execute_reply":"2024-04-09T09:18:22.890705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* loss: Binary cross-entropy loss function for binary classification.\n* optimizer: Adam optimizer for training the model.\n* metrics: Monitors the accuracy during training.","metadata":{}},{"cell_type":"markdown","source":"# Train the Model","metadata":{}},{"cell_type":"code","source":"model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:18:22.89358Z","iopub.execute_input":"2024-04-09T09:18:22.89401Z","iopub.status.idle":"2024-04-09T09:19:14.611227Z","shell.execute_reply.started":"2024-04-09T09:18:22.893979Z","shell.execute_reply":"2024-04-09T09:19:14.610262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* We train the model on the training data (X_train and y_train) for 10 epochs (epochs).\n* validation_data is used to monitor the model's performance on the validation set during training.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\n\nplot_model(model, to_file='model_architecture.png', show_shapes=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:19:14.613916Z","iopub.execute_input":"2024-04-09T09:19:14.614217Z","iopub.status.idle":"2024-04-09T09:19:14.758311Z","shell.execute_reply.started":"2024-04-09T09:19:14.614193Z","shell.execute_reply":"2024-04-09T09:19:14.757173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> TensorFlow provides tools for feature map visualization.--it shows the the convolutional layers","metadata":{}},{"cell_type":"markdown","source":"# Evaluate the Model","metadata":{}},{"cell_type":"code","source":"loss, accuracy = model.evaluate(X_test, y_test)\nprint('Test Loss:', loss)\nprint('Test Accuracy:', accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:19:14.759463Z","iopub.execute_input":"2024-04-09T09:19:14.759745Z","iopub.status.idle":"2024-04-09T09:19:15.219757Z","shell.execute_reply.started":"2024-04-09T09:19:14.759722Z","shell.execute_reply":"2024-04-09T09:19:15.218517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*We evaluate the model's performance on the unseen testing data (X_test and y_test).*\n\n*Test Accuracy of 86.27% indicates that the model correctly classified brain tumor presence or absence in 86.27% of the images in the testing set. This is a promising result, showing the model learned to identify patterns in the brain MRI images that are relevant to tumor detection.\n\nA lower loss value generally indicates better model performance. In binary cross-entropy loss (used here), a value closer to 0 signifies better classification between tumor and no-tumor classes. While 0.6356 isn't an exceptional loss, it suggests the model can differentiate between the classes to some extent.*\n\n*These results suggest that the CNN model has some potential for brain tumor detection. However, there's room for improvement. eg Hyperparameter Tuning or Data Augmentation:----- Artificially increasing the size and diversity of the training data using techniques like random flips, rotations, or zooms can help improve model generalizability and prevent overfitting.*","metadata":{}},{"cell_type":"markdown","source":"#  Build CNN Model using Functional API","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\nfrom tensorflow.keras.models import Model\n\n# Define input shape (assuming your images are 224x224 with 3 channels)\ninput_shape = (224, 224, 3)\n\n# Define the model layers\ninputs = Input(shape=input_shape)  # Input layer\n\n# Feature extraction layers\nx = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')(inputs)  # Convolutional layer 1\nx = MaxPooling2D(pool_size=(2, 2))(x)  # Max pooling layer 1\nx = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')(x)  # Convolutional layer 2\nx = MaxPooling2D(pool_size=(2, 2))(x)  # Max pooling layer 2\n\n# Flatten the feature maps\nx = Flatten()(x)\n\n# Classification layers\nx = Dense(units=128, activation='relu')(x)  # Fully connected layer 1\noutputs = Dense(units=1, activation='sigmoid')(x)  # Output layer with sigmoid for binary classification\n\n# Create the model\nmodel = Model(inputs=inputs, outputs=outputs)\n\n# Compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# Train the model (replace with your training data)\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:19:15.220845Z","iopub.execute_input":"2024-04-09T09:19:15.221169Z","iopub.status.idle":"2024-04-09T09:20:49.264478Z","shell.execute_reply.started":"2024-04-09T09:19:15.22114Z","shell.execute_reply":"2024-04-09T09:20:49.263647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Input Layer: Input(shape=input_shape) defines the model's input.\n\n* Conv2D: Applies convolutional filters to capture spatial patterns.\n* MaxPooling2D: Reduces dimensionality and introduces a degree of invariance.\n* Flatten Layer: Flattens the feature maps into a 1D vector before feeding them to fully connected layers.\n* Classification Layers: These layers perform classification.\n* Dense: Fully connected layers learn complex relationships between features.\n\n* units=1: One unit for binary classification (tumor or no tumor).\n* activation='sigmoid': Sigmoid activation outputs a probability between 0 and 1 (tumor).\n\n\n* Model(inputs=inputs, outputs=outputs) defines the model architecture.\n* model.compile(): Sets the loss function (binary cross-entropy for binary classification), optimizer (adam), and metrics (accuracy).\n\n* model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test)):\n* Trains the model on X_train (images) and y_train (labels) for 10 epochs (iterations).\n* Uses a batch size of 32 for training efficiency.\n* Evaluates performance on the validation set (X_test, y_test) during training.","metadata":{}},{"cell_type":"code","source":"test_loss, test_acc = model.evaluate(X_test, y_test)\nprint(\"Test Loss:\", test_loss)\nprint(\"Test Accuracy:\", test_acc)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:20:49.265446Z","iopub.execute_input":"2024-04-09T09:20:49.265718Z","iopub.status.idle":"2024-04-09T09:20:49.732201Z","shell.execute_reply.started":"2024-04-09T09:20:49.265695Z","shell.execute_reply":"2024-04-09T09:20:49.731597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train your model (replace with your training data and model)\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n\n# Extract training and validation loss/accuracy from history\ntrain_loss = history.history['loss']\nval_loss = history.history['val_loss']\ntrain_acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\n# Plot training and validation loss/accuracy\nepochs = range(len(train_loss))  # Get number of epochs\nplt.figure(figsize=(10, 6))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:23:03.780114Z","iopub.execute_input":"2024-04-09T09:23:03.780428Z","iopub.status.idle":"2024-04-09T09:24:36.618564Z","shell.execute_reply.started":"2024-04-09T09:23:03.780406Z","shell.execute_reply":"2024-04-09T09:24:36.617989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot loss\nplt.plot(epochs, train_loss, label='Training Loss')\nplt.plot(epochs, val_loss, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.figure(figsize=(10, 6))","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:26:43.382861Z","iopub.execute_input":"2024-04-09T09:26:43.383242Z","iopub.status.idle":"2024-04-09T09:26:43.621447Z","shell.execute_reply.started":"2024-04-09T09:26:43.383215Z","shell.execute_reply":"2024-04-09T09:26:43.620517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot accuracy\nplt.plot(epochs, train_acc, label='Training Accuracy')\nplt.plot(epochs, val_acc, label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-09T09:26:47.697301Z","iopub.execute_input":"2024-04-09T09:26:47.697597Z","iopub.status.idle":"2024-04-09T09:26:47.882582Z","shell.execute_reply.started":"2024-04-09T09:26:47.697575Z","shell.execute_reply":"2024-04-09T09:26:47.881814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note the difference between accuracy for sequential and functional API**\n\n*Functional API (Test Loss: 0.895, Test Accuracy: 0.863): This model seems to have learned some patterns but might be overfitting (high training and low validation performance) or might not be complex enough to capture crucial features in your brain tumor data.*\n\n\n*Sequential API (Test Loss: 0.582, Test Accuracy: 0.902): This model achieves significantly lower loss and higher accuracy, suggesting it might be a better fit for your dataset. *\n\n\n*The brain tumor dataset might have inherent characteristics that can be effectively captured by a simpler convolutional architecture (achieved by the Sequential API). This could be because the dataset might not be large enough or complex enough to necessitate the power of the Functional API for this specific task.*\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}